{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9024fc9e-f17d-4f15-b174-1f268d32f6af",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "In the Supervised Learning paradigm, we have a collection of observations (our data), each with a set of features and a particular variable of interest, i.e., the \"target\" (in regression) or \"label\" (in classification) which we aim to predict.\n",
    "\n",
    "In the **Unsupervised Learning** paradigm, there is no particular variable of interest. In this way, the goal is not to predict an aspect of, but rather **to organize the data**, and subsequently act on our organization. Most forms of unsupervised learning can be categorized as one of the following:\n",
    "\n",
    "- **Dimensionality Reduction.** Use relationships between rows and columns to represent the data with fewer columns.\n",
    "- **Clustering:** Group rows of data into categories based on their featuers (\"similar\" data are placed into similar groups).\n",
    "- **Recommendation.** Use the above two strategies to acertain what might be a good recommendation to someone with access to the features of data in question.\n",
    "\n",
    "And, all three of these are based on the notion of \"**similarity**\".\n",
    "\n",
    "*With unsupervised learning, we will mainly use the `.transform()` and `.fit_transform()` methods in Scikit-Learn that we saw in the lab on NLP methods.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a66fa9-578b-4a24-8b22-85d3ab3228a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(context='notebook', style='whitegrid', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d00ffb4-e1ed-4c13-a95a-1d0b25998aeb",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "Unsupervised Learning is still a form of \"machine\" learning, so we grant that our work is meant to generalize to data that is yet unseen. So, we should still have a hold out (or \"test\") set of data on which to evaluate our models later.\n",
    "\n",
    "In this lab, we'll be using a collection of beer review data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88fcab22-312e-467f-baaa-d39a58bc7171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf63b84-6c13-4cd8-995b-cbfec7beb06b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/beer_reviews.csv.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/jc80cx2x5tn05gttrc1j87780000gn/T/ipykernel_18267/3978928393.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we can use `read_csv` to read the compressed csv file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/beer_reviews.csv.zip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;31m# BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;31m# ReadBuffer[bytes], WriteBuffer[bytes]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             handle = _BytesZipFile(\n\u001b[0m\u001b[1;32m    741\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompression_args\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, archive_name, **kwargs)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;31m# TextIOBase, TextIOWrapper, mmap]]]\"; expected \"Union[Union[str,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;31m# _PathLike[str]], IO[bytes]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_zip\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfer_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1246\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1248\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1249\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/beer_reviews.csv.zip'"
     ]
    }
   ],
   "source": [
    "# we can use `read_csv` to read the compressed csv file\n",
    "df_raw = pd.read_csv(\"./data/beer_reviews.csv.zip\", compression='zip', low_memory=False)\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fc91613-8b18-4b0c-8ec1-cd454a0ea063",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_raw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/jc80cx2x5tn05gttrc1j87780000gn/T/ipykernel_18267/3388380306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_beer_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_holdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_raw' is not defined"
     ]
    }
   ],
   "source": [
    "df_beer_reviews, df_holdout = train_test_split(df_raw, test_size=0.20, random_state=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d1b8223-fe6a-426e-8c8a-5d6f9ba84f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_beer_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/jc80cx2x5tn05gttrc1j87780000gn/T/ipykernel_18267/981800867.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_beer_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_beer_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "df_beer_reviews.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3559aaef-7a0d-40f6-adb0-dfb9922d3c9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# there are not many NA values compared to all the data, so we drop them\n",
    "df_beer_reviews.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f43449-77fa-4f6f-96be-03f6b3fb8eb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_beer_reviews.to_csv('./data/beer_reviews.csv', index=False)\n",
    "# df_holdout.to_csv('./data/beer_reviews_holdout.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0675d33-541c-4792-a293-343257a00b69",
   "metadata": {},
   "source": [
    "## The Curse of Dimensionality\n",
    "\n",
    "As data sets become increasingly wide (i.e., more columns), they in turn become more unweildy. We may want as many columns as possible to maximize the information we have for each observation, but as we do so, we run into a couple of problems:\n",
    "\n",
    "1. more columns will increase the computational load on any algorithm acting on that data. \n",
    "2. high-dimensional spaces quickly become sparse, and require exponentially more data accurately represent a situation.\n",
    "\n",
    "This phenomenon is commonly called **\"The Curse of Dimensionality\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce17e61a-33e4-4264-bf55-1754c84c9b00",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "We can combat the curse of dimensionality using dimensionality reduction techniques. In dimensionality reduction, we extract latent features in our data that otherwise would go unnoticed (a.k.a., \"Feature Extraction\"). These latent features retain some acceptable percentage of the information from the original dataset, and we can use a fewer number of them in lieu of the wider dataset we started with. *Note: In a way, classification and regression are extreme forms of dimensionality reduction.*\n",
    "\n",
    "One of the most common dimensionality reduction techniques is one we've already seen: Principal Components Analysis (PCA) which uses Singular Value Decomposition (SVD). Recall that SVD is a guaranteed decomposition for any rectangular matrix. It creates the following equality:\n",
    "\n",
    "$$\n",
    "X = U\\Sigma V^\\top\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28348985-85f2-47ab-bb21-0c24a4627894",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7d375-4241-4b2d-bf9b-2f856299d94e",
   "metadata": {},
   "source": [
    "### Indexing Entities\n",
    "\n",
    "Here, we'll use PCA to capture the features of the beers which were reviewed. We'll reduce the dimensionality of our beer data, after converting categorical columns to numeric ones with `get_dummies`. But first, it is often helpful to create meaningful indices for our data. That is, we create a unique index for each row of data, and assign it to the `index` attribute of the data frame.\n",
    "\n",
    "We'll start by only considering beer-related columns of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df37bd34-a125-48e0-83b2-df9e602ee980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_beer_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/jc80cx2x5tn05gttrc1j87780000gn/T/ipykernel_18267/15500062.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_beers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_beer_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'beer_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beer_beerid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'brewery_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beer_style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m                           \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_overall'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beer_abv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_beer_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "df_beers = df_beer_reviews.groupby(['beer_name', 'beer_beerid', 'brewery_name', 'beer_style']) \\\n",
    "                          [['review_overall', 'beer_abv']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4743f49b-57ef-41e1-a18b-dc64ce13b2b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_beers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/73/jc80cx2x5tn05gttrc1j87780000gn/T/ipykernel_18267/3771877164.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_beers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_beers' is not defined"
     ]
    }
   ],
   "source": [
    "df_beers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d9098b-0c7b-4777-8a08-e4220b21f737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Number of `beer_beerids`: \", df_beers['beer_beerid'].nunique())\n",
    "print(\"Number of `beer_names`: \", df_beers['beer_name'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00cd3ff-8bd0-4f15-8392-9e053b9b069d",
   "metadata": {},
   "source": [
    "The \"names\" don't quite match up to their IDs, so we create a unique identifier for each row, and reassign it as the index for the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f506eee8-c836-4205-9ba4-b4e68a7144aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers['beer_id'] = df_beers['beer_name'] + ' (' + df_beers['beer_beerid'].astype(str) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0018cb5e-2c07-477e-bd68-ef0275b9efed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers['beer_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c72d8-1c32-4d4d-8455-fd4551b441b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers.set_index('beer_id', inplace=True)\n",
    "df_beers.drop(columns=['beer_beerid', 'beer_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63498b45-cca5-4788-b79d-e493fd9089b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eb46ca-9459-4cea-b7da-d4d594ab6bbd",
   "metadata": {},
   "source": [
    "Next, we'll reduce our scope to only the top 100 breweries, and the top 100 beer styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7457eaa9-625d-412a-8fe4-8c0dd99f0872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's just look at the top breweries and the top beer styles\n",
    "n_breweries = 100\n",
    "n_beer_styles = 100\n",
    "\n",
    "# `.value_counts` sorts the series descendingly by default\n",
    "brewery_popularity = df_beers.brewery_name.value_counts()\n",
    "top_breweries = brewery_popularity.iloc[:n_breweries].index\n",
    "\n",
    "beer_style_popularity = df_beers.beer_style.value_counts()\n",
    "top_beer_styles = beer_style_popularity.iloc[:n_beer_styles].index\n",
    "\n",
    "df_beers_top = df_beers[(df_beers.brewery_name.isin(top_breweries)) &\n",
    "                        (df_beers.beer_style.isin(top_beer_styles))].copy()\n",
    "\n",
    "df_beers_top.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c3ad5-4ff0-488d-ba3d-1da725abf59a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_num = pd.get_dummies(df_beers_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35110d55-da97-4aa0-903f-780aaf8d04f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_num.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b9cc03-f1d5-46b3-a318-400cb760840f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# proportion of the data which is zero\n",
    "(df_beers_num == 0).sum().sum() / (df_beers_num.shape[0] * df_beers_num.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f887b97-46d3-401f-8cf2-f00af25110a1",
   "metadata": {},
   "source": [
    "This is a pretty wide and sparse dataframe, so we use PCA to reduce the dimensinality. PCA decomposes the data into the following matrices: $U$, $\\Sigma$, $V^\\top$. In our case:\n",
    "\n",
    "- $U$ represents each beer in terms of \"beer-review flavor\" strengths; and we call these \"principal components\". Each beer-review flavor is a sort of mixture of the beer's brewery, beer style, ABV, and review overall. In some beer-review flavors, brewery plays more of a role than the others, in some flavors the ABV plays more of a role, and so on. The first (column) of these maintains the most variance or diversity.\n",
    "- $\\Sigma$ is a diagonal matrix (typically represented as a list of values), where each value represents the amount of variance \"explained\" by the corresponding beer-review flavor. *These values are typically presented in descending order.*\n",
    "- $V^\\top$ is very similar to $U$, but now we're representing each feature of the data in terms of beer-review flavors.\n",
    "\n",
    "`PCA()` is concerned primarily with $U$ and $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc0d8b-8227-42bf-a3e6-501fdf985b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "beers_pca_all = pca.fit_transform(df_beers_num)          # this is U\n",
    "exp_variances = pca.explained_variance_ratio_            # this is Sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0746b0e-a5ce-4795-bc17-2b1e94700459",
   "metadata": {},
   "source": [
    "But, we do not need all columns of $U$. We only need enough to capture a sufficient amount of variance explained in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb845a-4f4a-4de4-a464-7a54bfaa631d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot explained variance\n",
    "def plot_pca(exp_var_ratios, threshold = 0.85):\n",
    "    exp_var_cumsum = exp_var_ratios.cumsum()\n",
    "    t = (exp_var_cumsum < threshold).sum()\n",
    "    g = sns.lineplot(exp_var_cumsum)\n",
    "    plt.axvline(x=t, ls='--', color='gray', label=f\"{t} components\")\n",
    "\n",
    "    g.set_title(\"Variance Explained after PCA\")\n",
    "    g.set_xlabel(\"Principal Component\")\n",
    "    g.set_ylabel(\"Cumulative Explained Variance\")\n",
    "    g.legend();\n",
    "    \n",
    "    return g\n",
    "\n",
    "g = plot_pca(exp_variances, threshold = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff8ae0-2219-4673-a324-78d98dc7dd16",
   "metadata": {},
   "source": [
    "Suppose we are comfortable with a model which uses 85% of the variance in our data (in a way, this translates roughly to \"15% of the data can be considered outliers\"). In this case, we only need to keep 33 components. This is a significant improvement on the original 200+ columns of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24d6be8-7591-464a-978b-249c9d4d3dad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_pca = beers_pca_all[:, :33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c5d26-097b-4f62-8082-d85950fd7a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# these correspond to the rows of `df_beers_top`\n",
    "df_beers_pca = pd.DataFrame(beers_pca, index=df_beers_top.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cdf3f7-dfdd-40bb-934f-d5df6d82fb05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Similarity\n",
    "\n",
    "A foundational element of unsupervised learning is the notion of \"similarity.\"\" The way(s) in which this idea is defined will direct the rest of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca568b9-5102-406b-bf32-c9d25f0472c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a8939d-d27e-4f6e-b50c-46c5c48996d7",
   "metadata": {},
   "source": [
    "There are several similarity (or distance) metrics, but in this lab we'll investigate a few of the most common ones:\n",
    "\n",
    "- Euclidean Distance\n",
    "- Cosine Distance\n",
    "- Jaccard Distance\n",
    "- Manhattan Distance\n",
    "\n",
    "Note: \"Similarity\" is the complement of \"Distance\". Typically, similarity metrics exist between 0 and 1, so in general, $\\text{distance} = 1 - \\text{similarity}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773ef2e7-ab53-4c4f-baed-2dc71bf56bf4",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "The Euclidean Distance represents **the length of the line which connects two points in a vector space**. For two points on the x-y plane, it is the length of the hypotenuse of the triangle formed by the two points and a right angle between them. In general, for two points $p$ and $q$ in $k$-dimensional data, we have\n",
    "\n",
    "$$\n",
    "d_E(\\mathbf{p}, \\mathbf{q})={\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\cdots +(p_{k}-q_{k})^{2}}}\n",
    "$$\n",
    "\n",
    "*You can think of $\\mathbf{p}$ and $\\mathbf{q}$ as two rows of data, for instance.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772668fa-d5bb-4a52-822c-0657d14b4dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "points = [(1, 1),\n",
    "          (1, 2),\n",
    "          (4, 2)]\n",
    "\n",
    "dists = pairwise_distances(points, metric='euclidean', n_jobs=-1)\n",
    "df_dists = pd.DataFrame(data=dists, index=points, columns=points)\n",
    "df_dists\n",
    "# g = sns.heatmap(df_dists, annot=True, cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b2522-7146-4ba3-96ee-e4d35035cac4",
   "metadata": {},
   "source": [
    "### Cosine Distance\n",
    "\n",
    "If each point conotes a vector whose root is at the origin, and head is at the point, then the Cosine Similarity is **the cosine of the angle between the two vectors**. The Cosine *Distance* is 1 minus this value. In general, for two points in $k$-dimensional space, this is\n",
    "\n",
    "$$\n",
    "d_C(\\mathbf {p}, \\mathbf {q})=1 - \\cos(\\theta )=1 - {\\mathbf {p} \\cdot \\mathbf {q}  \\over \\|\\mathbf {p} \\|\\|\\mathbf {q} \\|}={\\frac {\\sum \\limits _{i=1}^{n}{p_{i}q_{i}}}{{\\sqrt {\\sum \\limits _{i=1}^{n}{p_{i}^{2}}}}\\cdot {\\sqrt {\\sum \\limits _{i=1}^{n}{q_{i}^{2}}}}}}\n",
    "$$\n",
    "\n",
    "In fact, this is the same thing as the (Pearson's) correlation coefficient between $\\mathbf{p} = \\mathbf{x_1} - \\overline{\\mathbf{x}}_1$ and $\\mathbf{q} = \\mathbf{x_2} - \\overline{\\mathbf{x}}_2$ for some vectors $\\mathbf{x}$ and $\\mathbf{y}$. *Why?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1393c-7c86-4788-9fc7-e70fe4f52548",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [(1, 1),\n",
    "          (1, 2),\n",
    "          (4, 2)]\n",
    "\n",
    "dists = pairwise_distances(points, metric='cosine', n_jobs=-1)\n",
    "pd.DataFrame(data=np.round(dists, 3), index=points, columns=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d6e4fe-6fb1-4daa-b348-1c70122c5f1d",
   "metadata": {},
   "source": [
    "### Jaccard Distance\n",
    "\n",
    "The [Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index) is a consideration of two vectors as if they were sets of items. It represents **the proportion of elements shared between two sets of data.** The more items they share (e.g., maybe each column represents a binary indicator of an item), the higher the Jaccard Similarity. *Note: This measure makes the most sense for binary data.* The Jaccard *Distance* is 1 minus this value:\n",
    "\n",
    "$$\n",
    "d_J(\\mathbf{p}, \\mathbf{q}) = 1 - {|\\mathbf{p} \\cap \\mathbf{q}| \\over |\\mathbf{p} \\cup \\mathbf{q}|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6cabe5-a930-4300-9e43-ce700df84bae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "points_tuples = [(0, 0, 1),\n",
    "                 (1, 1, 0),\n",
    "                 (1, 1, 1)]\n",
    "\n",
    "# jaccard similarity requires boolean data\n",
    "points = np.array(points_tuples).astype(bool)\n",
    "\n",
    "dists = pairwise_distances(points, metric='jaccard')\n",
    "pd.DataFrame(data=np.round(dists, 3), index=points_tuples, columns=points_tuples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2d22d-c4b7-4483-bc9c-d8db2b535367",
   "metadata": {},
   "source": [
    "### Manhattan Distance\n",
    "\n",
    "Manhattan Distance, or Taxicab Distance is **the sum of the perpendicular distances along the axes of a vector space, between two points.** So, in two dimensions, it is the total \"horizontal\" + \"vertical\" distance between two points.\n",
    "\n",
    "$$\n",
    "{\\displaystyle d_{\\text{T}}(\\mathbf {p} ,\\mathbf {q} )=\\sum _{i=1}^{n}\\left|p_{i}-q_{i}\\right|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3fe2d-45e6-4fce-83c6-ac665e31f348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "points = [(1, 1),\n",
    "          (1, 2),\n",
    "          (4, 2)]\n",
    "\n",
    "dists = pairwise_distances(points, metric='manhattan', n_jobs=-1)\n",
    "pd.DataFrame(data=np.round(dists, 3), index=points, columns=points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b210e-a5c2-4740-8a2c-cc1df898d18d",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "Suppose we had 100 rows of data with two columns: `purchases_per_month` and `spend_per_month`. Consider the following, for a possible scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98625435-4008-435b-a6f4-032142f5b76e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "centers = [(5, 5), (10, 10)]\n",
    "\n",
    "blobs, blob_labels = make_blobs(n_samples=100, n_features=2, cluster_std=1.0,\n",
    "                                centers=centers, shuffle=False, random_state=42)\n",
    "\n",
    "g = sns.scatterplot(x=blobs[:, 0], y=blobs[:, 1], hue=blob_labels, palette='Dark2')\n",
    "sns.scatterplot(x=[10], y=[6], label='noise?', color='black')\n",
    "\n",
    "g.set_xlabel('$ Spent')\n",
    "g.set_ylabel('Purchases per Month')\n",
    "g.set_title(\"(Made Up) Monthly Customers\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5e092a-9701-4983-a2ae-1de309d37c51",
   "metadata": {},
   "source": [
    "Visually, we can divide these data into two groups based on their features. Maybe the bottom group represents \"frugal\" customers, and the top group could be \"lavish\" customers. But, computationally, the question is a bit more difficult, *especially* when it comes to data with more than three columns.\n",
    "\n",
    "The purpose of clustering is to use the features of our data to computationally assign each observation to a group, based on some kind of similarity metric. Technically, the term **clustering** refers to grouping data in such a way there could be points leftover; we may call these points \"noise\". **Partitioning** on the other hand, partitions the data in such a way that *every* point is assigned to one partition. We typically use the term \"clustering\" to accont for both of these.\n",
    "\n",
    "In this lab, we will cover the k-Means algorithm, which is still one of the most scalable and interpretable algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470a7f4-bd57-4115-b33e-6411283a44bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5d295-3410-41a5-ac7c-403208ecd401",
   "metadata": {},
   "source": [
    "### k-Means\n",
    "\n",
    "k-Means clustering is a **partitioning** algorithm that divides data into $k$ clusters. Points are assigned to a cluster based on similarity to nearest cluster centroid. The value of $k$ is chosen by the user as a hyperparameter for the algorithm.\n",
    "\n",
    "    1. Choose `k` centroids (e.g., randomly)\n",
    "    2. Assign points to cluster based on nearest centroid\n",
    "    3. Recompute centroids based on each cluster's \"average\" point\n",
    "    4. Repeat steps (2) and (3) until algorithm converges\n",
    "\n",
    "*Interestingly, the boundary regions for KNN can always be reduced to a specific [Vonoroi Diagram](https://en.wikipedia.org/wiki/Voronoi_diagram).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0dbb4c-7be0-4226-831c-ffb9db355e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# suppose we start with 5 clusters (arbitrary at the start)\n",
    "num_clusters = 5\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, n_init='auto')\n",
    "km.fit(df_beers_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15e7ba-296b-4d27-a26b-1b24129ebb8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_top['cluster_kmeans'] = km.labels_.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be31bc-263d-4142-ae93-0d59ac44cdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beers_top[['beer_style', 'cluster_kmeans']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0d92f-c15c-42c8-b0f3-2caccf46e49b",
   "metadata": {},
   "source": [
    "**Strengths:**\n",
    "1. Simple parameter ($k$ clusters)\n",
    "2. Relatively fast for $n$ points in $d$-dimensions. The runtime is $O(nkdi)$ where $i$ is number of iterations until convergence.\n",
    "3. Guaranteed to converge.\n",
    "4. Easy to implement.\n",
    "\n",
    "**Weaknesses:**\n",
    "1. Optimal $k$ is often not obvious.\n",
    "2. Can get trapped in local minima (initial conditions matter).\n",
    "3. Sensitive to outliers (partitioning not clustering).\n",
    "4. Scaling affects results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf7e53-9393-4e40-b74d-64c8393825dc",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "There are many different ways to evaluate a clustering algorithm, but here we'll just discuss the following methods:\n",
    "\n",
    "- Inertia\n",
    "- Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d3c530-b3af-437e-8b4e-2bca4d466654",
   "metadata": {},
   "source": [
    "#### Inertia\n",
    "\n",
    "Inertia roughly translates to \"good clustering = points are close to cluster centroids\". In essence, we calculate the distances between points and their assigned centroids, and call this \"inertia\". In fact, this is the main goal of the k-Means algorithm.\n",
    "\n",
    "$$\n",
    "I = \\sum_{i = 0}^n\\min_{\\bar{x}_j\\in C}\\left(\\|x_i - \\bar{x}_j\\|^2\\right)\n",
    "$$\n",
    "\n",
    "where $x_i$ is each point in the data, $\\bar{x}_j$ represents the \"average\" point of cluster $j$, and $C$ is the colleciton of all clusters. We want $I$ to be as small as possible. *Note: $I \\geq 0$, and $I = 0$ only when all points lie at the same location, or the number of clusters = number of points.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf4e2d-485f-4f1d-b500-216aa4a43d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "km.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98462b-46e2-4908-a86c-41fecd1283b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inertias = []\n",
    "\n",
    "for num_clusters in range(2, 12):\n",
    "    km = KMeans(n_clusters=num_clusters, n_init='auto')\n",
    "    km.fit(df_beers_pca)\n",
    "    \n",
    "    inertias.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d3cd86-0625-4f26-94ef-b91e24a90682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.lineplot(x=range(2, 12), y=inertias)\n",
    "g.set_xlabel('Number of Clusters')\n",
    "g.set_ylabel(\"Intertia\")\n",
    "g.set_title(\"k-Means Intertia over k Clusters\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1400775e-02f9-4f18-9ca8-4f68ac6991f5",
   "metadata": {},
   "source": [
    "A good rule of thumb for picking the best value in a situation like this is to use the \"elbow\" method. Essentially, this is the point at which the intertia decreases with diminishing returns. In our case, it looks like our selection of $k=5$ clusters is best for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29061483-0492-4da4-8bac-cc3456f22013",
   "metadata": {},
   "source": [
    "#### Plotting\n",
    "\n",
    "Of course, a great method is directly plotting the relationships between the clusters you've build and the data. **The plots are up to you, and depend on your data**, but here we'll look at just the top styles of beer, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00490c2f-b0d3-4375-9218-01d78ecb847e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top 5 breweries\n",
    "mask = df_beers_top['beer_style'].value_counts().index[:5]\n",
    "mask = df_beers_top['beer_style'].isin(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b322427-503f-4665-8f1a-d418dafc6793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "g = sns.histplot(data=df_beers_top[mask].sort_values('cluster_kmeans'),\n",
    "                 x='cluster_kmeans', \n",
    "                 hue='beer_style', palette='Set1', multiple='stack')\n",
    "\n",
    "g.set_title('Distribution of Top Beer Styles Across Clusters')\n",
    "\n",
    "sns.move_legend(g, \"upper left\", bbox_to_anchor=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0007b373-f93b-49df-a31b-af93180a86e1",
   "metadata": {},
   "source": [
    "Here, we can see the some styles of beer are more prevalent in some clusters than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53b7ba-3bf5-4d27-991f-c90a02b827f8",
   "metadata": {},
   "source": [
    "## Recommender Systems\n",
    "\n",
    "A recommender system is an automated system that seeks to suggest whether a given item will be desirable to a user. These are algorithms that aim to provide the most relevant items to a user by **filtering useful information from noise.** For example, suppose there are too many books at the bookstore to browse at once. Let's say I’ve bought a few books related poetry. In this case, a book recommender system might:\n",
    "\n",
    "    1. Log the interest in poetry\n",
    "    2. Review other customers’ interests\n",
    "    3. Reference meta-information on all its books and customers\n",
    "    4. Suggest a set of products\n",
    "    \n",
    "There are two kinds of recommendation:\n",
    "\n",
    "- Content-Based Filtering\n",
    "- Collaborative Filtering\n",
    "\n",
    "We can also combine these two into a sort of \"hybrid\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19555858-11f2-4c4f-b327-a55aef2a90d7",
   "metadata": {},
   "source": [
    "### Content-Based Filtering\n",
    "\n",
    "With Content-Based Filtering, we define relevance using only item information. All we’d need is a matrix of items and their attributes. I.e., we project items into their feature space. The recommendation comes from item similarity. In this case, our definition of similarity is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3e4fe-9588-4e03-8358-b9dcf0adc0cd",
   "metadata": {},
   "source": [
    "#### Calculate Pairwise Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cc832-f527-4480-a081-36f7fbf3f25e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try metric='euclidean' and metric='cosine', and see what changes!\n",
    "dists = pairwise_distances(df_beers_pca, metric='cosine')\n",
    "dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f55e85-2fa9-4be0-a535-d270e7285f1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Numpy outputs an array here, so we want to get the names of the beers back\n",
    "dists = pd.DataFrame(data=dists, index=df_beers_pca.index, columns=df_beers_pca.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589239e-0439-437f-b304-30b3673fc109",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dists.iloc[0:5, 0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7384ab-7bab-4932-b0ca-7923355437e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Devise \"Ranking\" Scheme\n",
    "\n",
    "**Here is where we can get creative!**\n",
    "\n",
    "Let's say that a user has three beers that they like. We're going to say that a 'likeable' beer is one which is relatively close to all three of these beers. That is, the *sum* of the distances bewteen the 'likeable' beer and the three liked beers is minimal.\n",
    "\n",
    "Start by selecting three beers and store them in `beers_i_like` then look their distances to other beers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ab67f6-ba72-411c-a848-6c7a9fb94aed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_i_like = ['Belgian Blonde (44506)', 'ÜberFest Pilsner (39361)', '\"Cellar Reserve\" Triple Gold (25108)']\n",
    "dists[beers_i_like].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e87188-696a-4297-9476-f9c3bcb3768c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we sum each of the distances to the favorite beers by row. That is, we are summing 3 numbers for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca9b55a-0343-41b0-922c-da483626c6fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_summed = dists[beers_i_like].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800597c1-46f4-4713-9abc-1875d7df0226",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_summed = beers_summed.sort_values(ascending=True)\n",
    "beers_summed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def5d100-836c-471c-94aa-08dbaebe0d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter out the beers used as input using `.isin()`\n",
    "mask = ~beers_summed.index.isin(beers_i_like)\n",
    "ranked_beers = beers_summed.index[mask]\n",
    "ranked_beers = ranked_beers.tolist()\n",
    "\n",
    "ranked_beers[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f6ba6a-f40d-49cc-b43b-53b998838312",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "One way to evaluate a recommender system such as this one is to compare the features of the preferred items to the ones which have been recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288a841-ff1b-4a75-ad51-9d7c793091d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beers_i_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c67937-b601-43a3-8820-f65ac7a18201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top 5 recommendations\n",
    "recommendations = ranked_beers[:5]\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f1308-7d86-4439-9edc-7895a922f8b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eval = df_beers.copy()\n",
    "\n",
    "# Use `np.where` to create a categorical variable based on boolean series\n",
    "df_eval['rec_label'] = np.where(df_eval.index.isin(beers_i_like), 'Like',\n",
    "                       np.where(df_eval.index.isin(recommendations), 'Recommended',\n",
    "                       'Other'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e08d79-f25d-4e21-b91a-3c7c9332ed98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_eval[df_eval.rec_label.isin(['Like', 'Recommended'])] \\\n",
    "        .sort_values('rec_label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708998ab-59aa-4d7b-9944-bd34936d4495",
   "metadata": {},
   "source": [
    "In a way, this makes sense. Let's put this into a function, and see how things change with new beers and distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02d06e-dbe6-4cf6-9105-d5a0d31ec460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recommend_beers(beers_i_like, df_pca, num_recs=5, metric='euclidean'):\n",
    "    # calculate pairwise distances\n",
    "    dists = pairwise_distances(df_pca, metric=metric)\n",
    "    dists = pd.DataFrame(data=dists, index=df_pca.index, columns=df_pca.index)\n",
    "    \n",
    "    # ranking scheme\n",
    "    beers_summed = dists[beers_i_like].sum(axis=1)\n",
    "    beers_summed = beers_summed.sort_values(ascending=True)\n",
    "    \n",
    "    # remove preferred beers\n",
    "    ranked_beers = beers_summed.index[~beers_summed.index.isin(beers_i_like)]\n",
    "    ranked_beers = ranked_beers.tolist()\n",
    "    \n",
    "    # get recommendations (top-ranked)\n",
    "    recommendations = ranked_beers[:num_recs]\n",
    "    print(\"Recommendations: \")\n",
    "    print('\\t' + '\\n\\t'.join(recommendations))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def rec_eval(beers_i_like, recommendations, df_features):\n",
    "    df_eval = df_features.copy()\n",
    "\n",
    "    df_eval['rec_label'] = np.where(df_eval.index.isin(beers_i_like), 'Like',\n",
    "                                    np.where(df_eval.index.isin(recommendations), 'Recommended',\n",
    "                                             'Other'))\n",
    "    \n",
    "    df_eval = df_eval[df_eval.rec_label.isin(['Like', 'Recommended'])] \\\n",
    "                        .sort_values('rec_label')\n",
    "    \n",
    "    return df_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d174e4b-3a0d-438c-b3db-7b6a26324677",
   "metadata": {},
   "source": [
    "With Euclidean distance ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d07d62f-fe7f-468d-b8ba-da97a324f403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommendations = recommend_beers(beers_i_like, df_pca=df_beers_pca, metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428e6994-46bf-4e97-bb51-aced3df0af15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rec_eval(beers_i_like, recommendations, df_beers_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16410c49-b504-4c30-ac86-1af258ee4ce5",
   "metadata": {},
   "source": [
    "With Cosine distance ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a180e5-ff40-4bc0-a9a3-2e5750bde265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommendations = recommend_beers(beers_i_like, df_pca=df_beers_pca, metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbc728-1c60-428d-92ef-2481d8d2de5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rec_eval(beers_i_like, recommendations, df_beers_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ffde8-0203-4680-a073-0901c643e13e",
   "metadata": {},
   "source": [
    "**Caveats of Content-Based Filtering**\n",
    "\n",
    "- Recommendations biased toward past user preference\n",
    "    - Consider the feedback loop …\n",
    "- Suffers from “the cold start problem”\n",
    "    - **Item recommendations need examples; no data → “cold start”**\n",
    "    - If you have no product data, recommendations will be mal-informed\n",
    "- Diversity of recommendation space is diminished\n",
    "    - We can only work within neighborhoods of past items\n",
    "- Cross domain recommendations are difficult\n",
    "    - Think about recommending movies based on podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1739db8a-bb2f-49ae-ae16-b017f6c1e713",
   "metadata": {},
   "source": [
    "### Collaborative\n",
    "\n",
    "Here, we use other users’ history to predict new users’ preference. We define relevance using only user-item relationships, based on a user-item matrix containing “ratings” as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c790d-d24d-4a01-bba0-dc98e450bbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's restrict our dataset to the top reviewers.\n",
    "n = 100\n",
    "\n",
    "# The number of beers reviewed for each user\n",
    "reviewer_counts = df_beer_reviews.groupby('review_profilename')['beer_beerid'] \\\n",
    "                                 .nunique().sort_values(ascending=False)\n",
    "\n",
    "top_n_reviewers = reviewer_counts.index[:n]\n",
    "\n",
    "# this is the data we want\n",
    "df_filtered = df_beer_reviews[df_beer_reviews['review_profilename'] \\\n",
    "                              .isin(top_n_reviewers)].copy()\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ac7af9-5573-41af-b227-2768ada0b307",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_filtered[['beer_beerid', 'beer_name', 'review_profilename', 'review_overall']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9553abc-545e-4cff-8103-c8559742ac03",
   "metadata": {},
   "source": [
    "#### Collect Users as Features\n",
    "\n",
    "What we want is a matrix with users on one axis (e.g., columns), and items on the other axis (e.g., rows). Here, we start by grouping our data to get an aggregate reprsentative value for each item-user combination, and then use `unstack` to widen the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ffa5de-8e8b-43e7-92d9-09fd576a5147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a useful index\n",
    "df_filtered['beer_id'] = df_filtered['beer_name'] + ' (' + df_filtered['beer_beerid'].astype(str) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085abd06-1054-4543-8fc7-b46804ab9ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each beer (row), what is the average review score for each user (column)\n",
    "df_beer_user = df_filtered.groupby([\"beer_id\", \"review_profilename\"])['review_overall'] \\\n",
    "                          .mean()\n",
    "df_beer_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d54da-29bb-43c9-9937-e4400d677c2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert to a wide version\n",
    "df_beer_user = df_beer_user.unstack()\n",
    "df_beer_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdb8a8-055c-435a-bb70-9e3088eebd93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beer_user.isna().sum() / df_beer_user.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca4f65-a03f-4520-9bdb-61eddac1ece1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**How should we handle missing values here?** What does it mean for a beer *not* to be reviewed by a user? Should we treat these the same as the situations where a user reviewed a beer, and rated it zero? What if we used a negative number, how would that affect our distances/similarities between beer-user entities? What if we used the average value for each column, can we assume that a beer unreviewed by user $x$ will have their average beer rating?\n",
    "\n",
    "What if we assume (for the time being) that an unreviewed beer has an (impossible) rating of -1. This way, we can capture the fact that user hasn't reviewed that beer. It also comes in handy later when trying to spot the beers that have been rated vs. those which have not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5e8aa-2e0c-4d62-b6db-c3d7cd1abece",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_beer_user.fillna(-1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7ec909-cd00-4791-bac6-62dc7d5eec67",
   "metadata": {},
   "source": [
    "Recall that Singular Value Decomposition (SVD) is a method we can use to understand latent \"topic directions\" in text data. So, given a document-term matrix, we can get an idea for the kinds of topics that exist in the data (i.e., the ways words are used in documents). With our user-item data, we can use SVD to understand sort of \"*usage* directions\". In this way, given a user-item matrix, we get an idea for the ways items are used by users given our data set.\n",
    "\n",
    "<center>\n",
    "    <img width='65%' src=\"http://zwmiller.com/projects/images/svd_breakdown.png\">\n",
    "</center>\n",
    "\n",
    "Traditionally, we'll want to reduce our number of *usage directions* (i.e., the number of columns in the middle matrix on the right of the equals sign, above) to some number less than the total number of users in our data set. This way, we can look at our data from the perspective of groups (the directions) and weights with which they're associated (how strongly a user/item is correlated in a direction). *In fact, we can use these weights to assign users (or items, for that matter) to groups.*\n",
    "\n",
    "[SciPy](https://docs.scipy.org/doc/scipy/index.html) is another Python library we will see more of in a later lesson. It's built to manage all kinds of mathematical operations such as linear algebra, signal processing, etc. Here, we'll use it's implementation of SVD so that we can capture all three of these matrices $U$, $\\Sigma$, and $V$, since SKLearn's `PCA` only gives us the first two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8a306-471e-4b65-b27b-3d964d86e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3171b-aa41-4bde-99d3-5fa46f2b785d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "beer_usage, usage, usage_user = svd(df_beer_user, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220d931-1ed7-4bda-a73d-1d2f4928c3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each beer, how much of their use (rating) is captured by each usage type?\n",
    "df_beer_usage = pd.DataFrame(data=beer_usage, index=df_beer_user.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a405af-3d94-48b6-bb59-ac9b00f37082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For each user, how much of their activity is captured by each usage (rating scheme) type?\n",
    "df_usage_user = pd.DataFrame(data=usage_user, columns=df_beer_user.columns)\n",
    "df_user_usage = df_usage_user.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03346a9c-b02a-4bf6-ac16-2e3a01d820fb",
   "metadata": {},
   "source": [
    "- `df_beer_usage` is now a matrix where each row represents a beer, and each column represents a kind of *usage*. If we look at any row (beer), the columns tell us how much information about that beer can be gathered solely by looking at it's usage along each usage type\n",
    "- `df_user_usage` is a matrix where each row is a kind of usage, and the columns are users. Similar to the above, it tells us how much information about that user can be gathered just by looking at their activity in some usage type.\n",
    "- `usages` contains the sort of variety of users/beers that are represented by each usage type. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68508588-6a9c-4473-89c1-4719d1903b98",
   "metadata": {},
   "source": [
    "#### Similar User Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c94f25-4add-4e81-a6a2-d9db72917ba8",
   "metadata": {},
   "source": [
    "Recall that the `usages` came from `svd`, and it represents the singular values (the *weights*) of the Singular Value Decomposition. Each singular value tells us *how much* a principal component (\"usage type\") tells us about the user or item. So, naturally, if we take any singular value `usage[i]` and divide it by the sum of all the usages `usage[i] / usage.sum()`, we have the amount of \"explained variance\" that the `i`th singular value provides. \n",
    "\n",
    "Also, `usages` is **naturally sorted** in descending order by the `svd` algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cf5448-7574-494b-8e7a-9e908d8e9dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot the explained variance of each of these usage types\n",
    "exp_var_ratios = usage / usage.sum()\n",
    "\n",
    "g = plot_pca(exp_var_ratios, threshold = 0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c861aef9-2192-4913-a90e-7c09b0d2d5ed",
   "metadata": {},
   "source": [
    "Notice that it takes many more principle components to capture 85% of the variance in this data. In other words, there are more than 80 different ways to review beer that are relatively distinct from one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c91b5-beb3-42fc-b956-ce67abbea767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To keep 85% of the variance, let's consider the first 80 usage types (principal components)\n",
    "dists_users = pairwise_distances(df_user_usage.iloc[:, :80], metric='euclidean')\n",
    "dists_users = pd.DataFrame(dists_users, index=df_user_usage.index, columns=df_user_usage.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cc335e-d746-4e02-b3da-1ab0c09b1b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_user_neighbors(user, neighborhood=10):\n",
    "    '''\n",
    "    Given a user, return the closest users by the `dists_users` matrix\n",
    "    '''\n",
    "    neighbors = dists_users[user].sort_values()[:neighborhood]\n",
    "    \n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a72d26e-b37f-4d26-9644-a91474a40239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_user_usage.index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c483bd33-77d6-458d-810b-a53771506f8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for example\n",
    "neighbors = get_user_neighbors('BEERchitect')\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08f286-27aa-4090-a422-1c1323525375",
   "metadata": {},
   "source": [
    "**Diverse Reviewers**\n",
    "\n",
    "First, we can take a look at the beer ratings for top users, and those closest to them using usage types (from `dists`, above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64864892-c956-4880-b37c-30319845c06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count non-negative values (i.e., beers with reviews), and sort\n",
    "top_users = (df_beer_user > 0).sum(axis=0) \\\n",
    "                              .sort_values(ascending=False).index[:10]\n",
    "top_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224fcaeb-d7cb-4177-a582-4d2f3231510f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_usage(user, neighborhood=10):\n",
    "    # Get the `neighborhood` closest neighbors\n",
    "    neighbors = get_user_neighbors(user, neighborhood).index.tolist()\n",
    "    \n",
    "    # build array of review values for beers reviewed by neighbors\n",
    "    mask = df_filtered['review_profilename'].isin(neighbors)\n",
    "    df_plot = df_filtered[mask][['review_profilename', 'beer_style', 'review_overall']]\n",
    "    df_plot = df_plot.groupby(['beer_style', 'review_profilename'])['review_overall'] \\\n",
    "                     .mean().unstack()\n",
    "\n",
    "    df_plot = df_plot.loc[:, neighbors] \\\n",
    "                     .sort_values(user, ascending=False) \\\n",
    "                     .dropna(how='all')\n",
    "    \n",
    "    # We'll only annotate the heatmap if the text is easier to see\n",
    "    df_annot = df_plot.round(2) if df_plot.shape[0] <= 20 else None\n",
    "    \n",
    "    # We're sure to be intentional about a \"center\" point\n",
    "    sns.heatmap(df_plot,\n",
    "                annot=df_annot,\n",
    "                cmap='Blues',\n",
    "                center=3);  # Maybe below a 3 is bad, above is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e7d23-acc5-4a9d-95ad-75159cd7d0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_usage('mikesgroove')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2803ade-c48d-4518-9864-57546ac64b13",
   "metadata": {},
   "source": [
    "Based off this, could we conclude this is a good recommender?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795b2dd8-c87d-44b8-8093-9c986a4a3e58",
   "metadata": {},
   "source": [
    "#### Recommendation\n",
    "\n",
    "Say for some user, we take the top `m` closest users, and recommend `r` different beers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bde74da-81c2-4739-8344-2534c45aaeb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = 'dyan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7237f0-d4c5-4f7f-b659-62776e94f8ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We consider the 20 closest users (reviewers) to choose untried beers\n",
    "n_neighbors = 20\n",
    "\n",
    "neighbors = get_user_neighbors(user, n_neighbors+1)\n",
    "neighbors = neighbors[1:]  # We don't want to include the user themself\n",
    "neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530400d5-c77b-491d-a9ee-5eac475a26cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Look at *untried* beers among the neighboring users\n",
    "untried_beers = df_beer_user[user] < 0\n",
    "df_rec = df_beer_user.loc[untried_beers, neighbors.index].copy()\n",
    "\n",
    "df_rec = df_rec.replace(-1, np.nan)  # Replace the un-rated beers with NAN values\n",
    "df_rec = df_rec.dropna(how='all')    # Only keep rows where at least one neighbor rated the beer\n",
    "\n",
    "df_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8504a8a-fefe-47a2-bdbe-89872b72e7b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the average rating among the top `n_neighbors` of our user (HOW COULD WE SMARTER ABOUT THIS?)\n",
    "avg_beer_rating = np.nanmean(df_rec, axis=1)\n",
    "avg_beer_rating = pd.Series(index=df_rec.index, data=avg_beer_rating)\n",
    "avg_beer_rating.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# Return up to `n_recommendations`\n",
    "n_recommendations = 5\n",
    "avg_beer_rating[:n_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5f830-5c1a-4eb0-a896-c9821cc17758",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Put all this into a function\n",
    "def get_recommendations(user, n_recommendations=5, n_neighbors=20):\n",
    "    neighbors = get_user_neighbors(user, n_neighbors+1)\n",
    "    neighbors = neighbors[1:]  # We don't want to include the user themself\n",
    "    \n",
    "    untried_beers = df_beer_user[user] < 0\n",
    "    df_rec = df_beer_user.loc[untried_beers, neighbors.index].copy()\n",
    "\n",
    "    df_rec = df_rec.replace(-1, np.nan)  # Replace the un-rated beers with NAN values\n",
    "    df_rec = df_rec.dropna(how='all')  # Only keep rows where at least one neighbor rated the beer\n",
    "    \n",
    "    # Get the average rating among the top `n_neighbors` of our user\n",
    "    avg_beer_rating = np.nanmean(df_rec, axis=1)\n",
    "    avg_beer_rating = pd.Series(index=df_rec.index, data=avg_beer_rating)\n",
    "    avg_beer_rating.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "    # Return up to `n_recommendations`\n",
    "    return avg_beer_rating[:n_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b04d8-14bb-4fa3-80c0-459309b2aa55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = 'Thorpe429'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3817417f-f0d8-4f03-8093-214a7a2045fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_recommendations(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d83a6f-7016-44e3-9898-2db488cd9944",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4cf24ae-9b65-4a35-9c7b-06f5ad047a11",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b530a699-ec24-487a-948e-f33802dedaa6",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Write a function called `euclidean` which calculates the Euclidean distance between two points (of any `k` dimensions), using only vanilla Python (no packages) and NumPy. This should return a single value.\n",
    "\n",
    "If you can, adjust your function to calculate *pairwise* Euclidean distances for any number of points (i.e., return a matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9b909",
   "metadata": {},
   "source": [
    "$$\n",
    "d_E(\\mathbf{p}, \\mathbf{q})={\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\cdots +(p_{k}-q_{k})^{2}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8eab4",
   "metadata": {},
   "source": [
    "### Single Pair Euclidean Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20bad90a-81b3-4e23-ad67-3dba5a075374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian(points):\n",
    "    summation = 0\n",
    "    dimensions = len(points[0])\n",
    "    for i in range(dimensions):\n",
    "        squ_diff = (points[0][i] - points[1][i])**2\n",
    "        summation+= squ_diff\n",
    "    return summation**(1/2)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d3954",
   "metadata": {},
   "source": [
    "### Pairwise Euclidean Distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd517cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test = [[1,3,5],[6,3,5],[0,4,6],[4,5,9]]\n",
    "\n",
    "def euclidean_pairwise(points):\n",
    "    num_points = len(points)\n",
    "    dimensions = len(points[0])\n",
    "    rows = []\n",
    "    for i in range(num_points):\n",
    "        column = []\n",
    "        for p in range(num_points):\n",
    "            dist = euclidian([test[i],test[p]])\n",
    "            column.append(dist)\n",
    "        rows.append(np.array(column))\n",
    "        \n",
    "    return np.array(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "90eef96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 5.        , 1.73205081, 5.38516481],\n",
       "       [5.        , 0.        , 6.164414  , 4.89897949],\n",
       "       [1.73205081, 6.164414  , 0.        , 5.09901951],\n",
       "       [5.38516481, 4.89897949, 5.09901951, 0.        ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "euclidean_pairwise(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0b9be-4978-48bd-87ec-ddf48895669d",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Use your function from Exercise 1 and the `df_user_usage` dataframe to calculate the distance between any two users. Interpret your results for a random selection of two users compared to another selection of two *other* users.\n",
    "\n",
    "*Hint: you'll need to incorporate the fact that `df_user_usage` came **after** PCA dimensionality reduction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508e447",
   "metadata": {},
   "source": [
    "### Getting 2 random points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f0de6-6cc9-4c12-9b3f-38130f3eb604",
   "metadata": {},
   "outputs": [],
   "source": [
    "random = df_user_usage.sample(2)\n",
    "pt1 = tuple(random.iloc[0])\n",
    "pt2 = tuple(random.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7959ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9aae72",
   "metadata": {},
   "source": [
    "### Applying our Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ead5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidian([pt1,pt2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543c88ed",
   "metadata": {},
   "source": [
    "### Getting 2 different points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bacbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random2 = df_user_usage.sample(2)\n",
    "pt3 = tuple(random2.iloc[0])\n",
    "pt4 = tuple(random2.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ce871",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidian([pt3,pt4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe82a5",
   "metadata": {},
   "source": [
    "### Difference Between the points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e356905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidian([pt1,pt2]) - euclidian([pt3,pt4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84aecd2",
   "metadata": {},
   "source": [
    "### Interpretations:\n",
    "\n",
    "Since the difference is extremely small, this implies that the level of dimensionality has been effectively reduced by the Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19d3228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
